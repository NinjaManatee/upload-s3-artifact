# Mirrors action/upload-artifact, taking many of the same parameters, but uploads to S3 instead of GitHub.
# based on open-turo/actions-s3-artifact
# See: https://github.com/open-turo/actions-s3-artifact/blob/main/upload/action.yaml
name: Upload artifact(s) to AWS S3
description: Uploads artifact(s) to an AWS S3 bucket

# Inputs, outputs and descriptions pulled from actions/upload-artifact
inputs:
  name:
    description: Artifact name
    default: artifact
  path:
    description: A file, directory or wildcard pattern that describes what to upload
    required: true
  if-no-files-found:
    type: choice
    description: >
      The desired behavior if no files are found using the provided path.

      Available Options:
        warn: Output a warning but do not fail the action
        error: Fail the action with an error message
        ignore: Do not output any warnings or errors, the action does not fail
    options:
      - warn
      - error
      - ignore
    default: warn
  retention-days:
    description: >
      Duration after which returned URL will expire in days. 0 means using default retention. This is only for the 
      presigned URL returned by the action and printed to the job summary. Controlling when an artifact is actually 
      deleted is done in the AWS S3 bucket configuration.
  
      Minimum 1 day.
      Maximum 90 days unless changed from the repository settings page.
      default is 7 days
    type: number
    default: 7
  compression-level:
    type: choice
    description: >
      The level of compression for Zlib to be applied to the artifact archive.
      The value can range from 0 to 9:
        - 0: No compression
        - 1: Best speed
        - 6: Default compression (same as GNU Gzip)
        - 9: Best compression
      Higher levels will result in better compression, but will take longer to complete.
      For large files that are not easily compressed, a value of 0 is recommended for significantly faster uploads.
    options:
      - 0
      - 1
      - 2
      - 3
      - 4
      - 5
      - 6
      - 7
      - 8
      - 9
    default: 6

  # TODO: Implement overwrite
  overwrite:
    description: >
      If true, an artifact with a matching name will be deleted before a new one is uploaded.
      If false, the action will fail if an artifact for the given name already exists.
      Does not fail if the artifact does not exist.
    default: false
    type: boolean
  include-hidden-files:
    description: >
      If true, hidden files will be included in the artifact.
      If false, hidden files will be excluded from the artifact.
    default: false
    type: boolean

outputs:
  artifact-url:
    description: >
      A download URL for the artifact that was just uploaded. Empty if the artifact upload failed.

      This download URL only works for requests Authenticated with GitHub. Anonymous downloads will be prompted to first login. 
      If an anonymous download URL is needed than a short time restricted URL can be generated using the download artifact API: https://docs.github.com/en/rest/actions/artifacts#download-an-artifact    

      This URL will be valid for as long as the artifact exists and the workflow run and repository exists. Once an artifact has expired this URL will no longer work.
      Common uses cases for such a download URL can be adding download links to artifacts in descriptions or comments on pull requests or issues.
  artifact-digest:
    description: >
      SHA-256 digest for the artifact that was just uploaded. Empty if the artifact upload failed.

runs:
  using: 'composite'
  steps:
    - name: Upload artifact
      shell: bash
      run: |
        # check whether AWS credentials are specified and warn if they aren't
        if [[ "${{ env.AWS_ACCESS_KEY_ID }}" == '' || "${{ env.AWS_SECRET_ACCESS_KEY }}" == '' ]]; then
          echo "::warn::AWS_ACCESS_KEY_ID and/or AWS_SECRET_ACCESS_KEY is missing from environment variables."
        fi
      
        # check whether S3_ARTIFACTS_BUCKET is defined
        if [[ "${{ env.S3_ARTIFACTS_BUCKET }}" == '' ]]; then
          echo "::error::S3_ARTIFACTS_BUCKET is missing from environment variables."
          exit 1
        fi

        # Script for URL encoding a string
        # see: https://gist.github.com/cdown/1163649#file-gistfile1-sh
        urlencode() {
            # urlencode <string>

            # save current LC_COLLATE for restoring
            local old_lc_collate=$LC_COLLATE

            # setting LC_COLLATE=C forces a case-sensitive sort, where 'A' comes before 'a'.
            LC_COLLATE=C

            # get the length of first argument
            local length="${#1}"

            local i
            for (( i = 0; i < length; i++ )); do
                # get the ith character from the input
                local c="${1:$i:1}"

                # print character if it doesn't need to be encoded, encode character if it does
                case $c in
                    [a-zA-Z0-9.~_-]) printf '%s' "$c" ;;
                    *) printf '%%%02X' "'$c" ;;
                esac
            done

            # restore original LC_COLLATE
            LC_COLLATE=$old_lc_collate
        }

        # Create our temporary directory parent for our artifacts
        TMP_ARTIFACT="$RUNNER_TEMP/upload-s3-artifact"
        if [[ "${{ runner.os }}" == "Windows" ]]; then
          # On some windows runners, the path for TMP_ARTIFACT is a mix of windows and unix path (both / and \), which
          # caused errors when un-taring. Converting to unix path resolves this.
          TMP_ARTIFACT=$(cygpath -u "$TMP_ARTIFACT")
        fi
        mkdir -p "$TMP_ARTIFACT"

        # Create a unique directory for this particular action run
        TMPDIR="$(mktemp -d -p "$TMP_ARTIFACT" "upload.XXXXXXXX")"
        echo "::debug::Created temporary directory $TMPDIR"

        # Assign the tarball file name for future use
        TMPTAR="$TMPDIR/artifacts.tgz"

        # Create a path within our temporary directory to collect all the artifacts
        TMPARTIFACT="$TMPDIR/artifacts"
        mkdir -p "$TMPARTIFACT"
        echo "::debug::Created artifact directory $TMPARTIFACT"

        echo "::debug::Input path: ${{ inputs.path }}"

        # Read the path string into a bash array for easy looping
        read -a ARTIFACT_PATHS <<< "${{ inputs.path }}"
        echo "::debug::Inputs read: $ARTIFACT_PATHS"

        # Iterate through each artifact path and copy it to the temporary path
        for name in ${ARTIFACT_PATHS[@]}; do
          if [[ -z "$name" ]]; then
            echo "::debug::Skipping empty"
            continue
          fi

          if [[ -n "$RUNNER_DEBUG" ]]; then
            echo "::debug::Contents of path"
            echo "$(tree -a "$name" 2>&1)"
          fi

          # check whether the path is an exclude and delete files in exclude from TMPARTIFACT
          if  [[ "$name" == ^!.* ]] ;
          then
            echo "::debug::Deleting $name"
            # remove first character
            name="${name:1}"

            # delete file
            # TODO: Is this working correctly? Do I want to be using "." here?
            relativePath=$(realpath --relative-to="." "$name")
            upperDir=".."
            if [[ "${relativePath#upperDir}" != "${relativePath}" ]]; then 
              echo "::error::Path $name isn't a subdirectory of the current directory! Not deleting."
            else
              rm -rf "$name"
            fi
          else
            echo "Adding '$name'"

            echo "::debug::Check if $name exists"
            if [[ -f "$name" ]]; then
              echo "::debug::$name exists"
              mkdir -p "$TMPARTIFACT/$(dirname "$name")"
              cp -r "$name" "$TMPARTIFACT/$(dirname "$name")"
            else
              case "${{ inputs.if-no-files-found }}" in
                "warn")
                  echo "::warn::$name does not exist"
                  ;;
                "ignore")
                  echo "::debug::$name does not exist"
                  ;;
                "error")
                   echo "::error::$name does not exist"
                    exit 1
                    ;;
              esac
            fi
          fi
        done

        # List out everything in the temporary path
        if [[ -n "$RUNNER_DEBUG" ]]; then
          echo "::debug::Contents of our temporary artifact build"
          if [[ "${{ runner.os }}" = "Windows" ]]; then
            cmd //c tree '$TMPDIR' /f
          else
            echo "$(tree -a '$TMPDIR' 2>&1)"
          fi
        fi

        # Tarball the temporary path into a single object
        echo "Creating artifact tarball"
        # exclude hidden files, if necessary
        if ! [[ "${{ inputs.include-hidden-files }}" ]]; then
          echo "::debug::Excluding hidden files."
          exclude=-'-exclude=".*"'
        fi

        # create tar
        echo '::debug::tar $exclude -cf "$TMPTAR" -I 'gzip -"${{ inputs.compression-level }}"' -C "$TMPARTIFACT" .'
        GZIP=-${{ inputs.compression-level }} tar $exclude -zcvf "$TMPTAR" -C "$TMPARTIFACT" .

        # original tar command from other repo. Am I missing something important? What does --transform and 
        # --show-transformed do?
        # tar -czvf "$TMPTAR" -C "$TMPARTIFACT" --transform='s/^\.\///' --show-transformed .

        # List the actual contents of the archive
        if [[ -n "$RUNNER_DEBUG" ]]; then
          echo "::debug::Artifact contents"
          echo '$(tar -ztvf "$TMPTAR" 2>&1)'
        fi

        # Get AWS S3 bucket URI and ensure it starts with "s3://"
        S3URI="${{ env.S3_ARTIFACTS_BUCKET }}"
        if [[ "$S3URI" != s3://* ]]; then
          echo "::debug::Adding s3:// to bucket URI"
          S3URI="s3://$S3URI"
        fi

        # Build key to object in S3 bucket
        REPO="${{ github.repository }}"
        RUN_ID="${{ github.run_id }}"
        ENCODED_FILENAME="$(urlencode ${{ inputs.name }}).tgz"
        KEY="$REPO/$RUN_ID/$ENCODED_FILENAME"
        S3URI="${S3URI%/}/$KEY"

        echo "Uploading '$TMPTAR' to S3 '$S3URI'"
        echo "::debug::aws s3 cp '$TMPTAR' '$S3URI'"
        aws s3 cp "$TMPTAR" "$S3URI"
        echo "::debug::File uploaded to AWS S3"

        # create presigned URL to download the artifact. AWS CLI expects expiration to be in seconds
        RETENTION_DAYS="${{ inputs.retention-days }}"
        EXPIRES_IN=$((RETENTION_DAYS * 24 * 60 * 60))
        echo "::debug::PRESIGNED_URL=\$\(aws s3 presign '$S3URI' --expires-in $EXPIRES_IN\)"
        # TODO: Presigned URL doesn't appear to be working correctly
        PRESIGNED_URL=$(aws s3 presign "$S3URI" --expires-in $EXPIRES_IN)
        echo "::debug::Presigned URL created: '$PRESIGNED_URL'"

        # create outputs and summary
        echo "artifact-url=$PRESIGNED_URL" >> $GITHUB_OUTPUT
        echo "artifact-urlartifact-digest=$(echo -n $TMPARTIFACT | sha256sum)" >> $GITHUB_OUTPUT
        NUM_BYTES=$(stat --printf="%s" "$TMPARTIFACT")
        echo "$NUM_BYTES"
        FORMATTED_BYTES=$(numfmt --to=iec $NUM_BYTES)
        echo "$FORMATTED_BYTES"
        echo "[${{ inputs.name }}]($PRESIGNED_URL)&nbsp;&nbsp;&nbsp;&nbsp;'$FORMATTED_BYTES'B" >> $GITHUB_STEP_SUMMARY

        # clean up temp dir
        rm -rf $TMP_ARTIFACT